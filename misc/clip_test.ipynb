{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/spektor/miniconda3/envs/transformers/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPModel, CLIPProcessor, CLIPFeatureExtractor, CLIPVisionModel\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "import requests\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCLIPModel(nn.Module):\n",
    "    def __init__(self, base_model, num_classes, hidden_size):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.text_projection = nn.Linear(512, 512)\n",
    "        self.visual_projection = nn.Linear(512, 512)\n",
    "        self.combine_projections = nn.Sequential(\n",
    "            nn.Linear(512, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, pixel_values):\n",
    "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask, pixel_values=pixel_values, return_dict=True)\n",
    "        text_proj = self.text_projection(outputs.text_embeds)\n",
    "        visual_proj = self.visual_projection(outputs.image_embeds)\n",
    "        combined = torch.cat((text_proj, visual_proj), dim=-1)\n",
    "        logits = self.combine_projections(combined)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-base-patch32 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_projection.weight', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'visual_projection.weight', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'logit_scale', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.final_layer_norm.weight']\n",
      "- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained CLIP model\n",
    "base_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "vision_model = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "# Define the number of output classes and hidden size for MLP\n",
    "# num_classes = 10\n",
    "# hidden_size = 128\n",
    "\n",
    "# # Initialize the custom model\n",
    "# model = CustomCLIPModel(base_model, num_classes, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/spektor/miniconda3/envs/transformers/lib/python3.9/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "transform = CLIPFeatureExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.config.text_config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transform' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m image_np \u001b[39m=\u001b[39m transform(image, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnp\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m'\u001b[39m\u001b[39mpixel_values\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mgi\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'transform' is not defined"
     ]
    }
   ],
   "source": [
    "image_np = transform(image, return_tensors=\"np\")['pixel_values'][0].transpose(1, 2, 0)\n",
    "print('gi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(text=[\"a photo o as aassf a cat\", \"asd asd ad sad sadad a\"], images=image, return_tensors=\"pt\", padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'pixel_values'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_output = vision_model(inputs['pixel_values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'pooler_output'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vis_output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50, 768])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vis_output['last_hidden_state'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vis_output['pooler_output'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = base_model(pixel_values=inputs['pixel_values'], input_ids=inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits_per_image', 'logits_per_text', 'text_embeds', 'image_embeds', 'text_model_output', 'vision_model_output'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0085, -0.0064, -0.0111,  ..., -0.0306, -0.0460, -0.0039],\n",
       "        [-0.0177,  0.0063,  0.0113,  ...,  0.0069, -0.0118,  0.0119]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.text_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in base_model.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0148,  0.0070, -0.0234,  ..., -0.0508, -0.0438,  0.0033],\n",
       "        [ 0.0087,  0.0258, -0.0387,  ..., -0.0547, -0.0242,  0.0112]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['text_embeds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-9.7877e-03,  1.2770e-02, -2.7419e-02,  1.9675e-03, -5.9326e-03,\n",
       "         -1.5613e-02, -1.2514e-02, -2.2667e-04,  4.3869e-02, -1.6322e-02,\n",
       "          2.2630e-02, -3.5160e-02,  4.4748e-03, -1.2946e-02, -3.1524e-02,\n",
       "         -1.1737e-02, -2.1543e-02, -2.7556e-02,  1.6562e-02,  4.5935e-03,\n",
       "         -1.2106e-01, -3.0035e-03,  3.9024e-02, -3.0893e-02, -4.3866e-03,\n",
       "          2.7598e-02,  2.2140e-02, -1.7065e-02,  1.4509e-02, -4.5195e-03,\n",
       "         -7.1843e-03,  2.3971e-02, -6.8107e-03,  1.6382e-02, -5.3629e-02,\n",
       "         -4.5550e-04,  2.5840e-02, -2.6581e-02,  1.7667e-02,  3.0216e-02,\n",
       "         -9.3064e-03, -3.2082e-02,  6.6351e-04, -1.3654e-02, -1.7603e-02,\n",
       "          5.3115e-05,  4.8170e-02,  1.3997e-02, -8.4859e-03,  1.6292e-02,\n",
       "          1.5116e-02,  2.3294e-02,  1.0750e-02, -4.9806e-03,  2.1177e-02,\n",
       "          1.7230e-02,  2.3855e-02,  5.5848e-02, -2.3911e-02, -1.5538e-02,\n",
       "          3.9247e-02, -1.2918e-02, -5.9023e-03,  3.2934e-02, -6.8747e-03,\n",
       "         -2.5089e-02,  2.0757e-02,  1.0150e-01,  1.8204e-02,  7.3074e-03,\n",
       "          1.8611e-02, -9.8677e-03,  2.5596e-02,  2.5613e-02,  4.5689e-02,\n",
       "         -1.5690e-03,  5.9213e-03, -3.1511e-02, -4.8854e-03, -1.2805e-02,\n",
       "         -1.7391e-02, -6.2625e-02, -4.2445e-02, -1.4618e-03, -5.1176e-02,\n",
       "          2.4318e-02,  3.9910e-02, -4.9757e-02,  2.7215e-02,  8.6429e-03,\n",
       "          1.6633e-02,  2.5690e-03, -6.7985e-01,  3.0191e-02, -1.9941e-02,\n",
       "          6.2129e-03,  1.6063e-02, -3.9373e-02, -8.5797e-02,  1.1725e-01,\n",
       "         -2.6975e-03,  5.6681e-03, -2.8281e-02, -6.9895e-04,  6.7088e-02,\n",
       "          2.3170e-02,  1.1389e-01,  2.4856e-02, -1.6714e-02, -4.3795e-02,\n",
       "         -1.4455e-02, -9.7788e-02,  5.7606e-03,  1.8513e-02,  1.7424e-02,\n",
       "          5.5291e-03,  7.9788e-03,  1.0814e-02,  1.6337e-02, -9.8479e-03,\n",
       "          1.5034e-02, -1.9465e-02, -1.6842e-02,  3.6695e-02, -1.5189e-02,\n",
       "         -8.6037e-03, -5.5819e-03,  4.8001e-02, -1.2025e-02,  2.6646e-03,\n",
       "         -4.9049e-03,  7.2539e-03, -1.8218e-02,  8.7776e-02, -3.8005e-02,\n",
       "         -3.5269e-02,  1.3749e-02, -4.5095e-02, -3.8029e-02, -1.2015e-02,\n",
       "          1.4632e-02, -3.4716e-02,  3.2748e-02,  3.8369e-02, -1.4159e-02,\n",
       "          2.6433e-02,  2.9841e-02, -5.2099e-02,  8.1901e-03,  2.2897e-02,\n",
       "         -4.4814e-02,  1.7407e-02, -2.8690e-02, -2.0881e-02,  1.8501e-03,\n",
       "         -5.4971e-03, -3.1056e-02, -4.4050e-02,  1.0940e-02, -3.7092e-02,\n",
       "         -2.5239e-03, -2.4367e-02,  1.0386e-02,  2.0601e-02,  3.0117e-05,\n",
       "         -3.5621e-02, -4.8163e-02,  4.8904e-02,  1.3468e-02,  2.8199e-02,\n",
       "          2.0676e-02,  6.3633e-02,  2.8261e-02, -2.0492e-02, -1.9600e-02,\n",
       "          1.7413e-02,  2.0422e-03,  5.0641e-03,  6.4531e-03,  2.3867e-02,\n",
       "         -3.4462e-02,  2.1516e-02, -1.1361e-02, -3.3557e-02, -5.4353e-03,\n",
       "          1.2138e-02,  1.5566e-02, -3.5140e-02,  3.0429e-02, -1.6446e-02,\n",
       "          3.2127e-02, -2.6737e-02, -5.0274e-02, -1.2288e-02, -4.5125e-02,\n",
       "         -3.0590e-03, -2.4329e-02, -3.2397e-02, -1.0299e-01, -3.9606e-02,\n",
       "          4.0710e-03,  3.3404e-02,  1.2409e-02,  1.7078e-02, -9.4017e-03,\n",
       "         -1.1334e-03,  3.0400e-02,  6.2409e-03, -2.8016e-02,  5.2085e-02,\n",
       "         -3.8158e-02,  4.7951e-02, -3.2811e-03,  3.0808e-02, -3.9396e-02,\n",
       "         -3.2622e-02, -2.6874e-04, -6.6302e-02,  7.6482e-02, -3.6597e-02,\n",
       "          3.3491e-04,  1.4367e-02, -1.8496e-02,  1.4942e-02, -4.6231e-03,\n",
       "          3.4127e-02,  1.3685e-02, -6.4290e-03,  2.2363e-03, -1.7988e-02,\n",
       "         -1.4226e-02,  4.7959e-03, -3.0834e-02,  7.0522e-02,  2.5589e-02,\n",
       "         -4.9408e-02, -3.2412e-02,  1.2270e-02,  1.3783e-02, -1.7417e-02,\n",
       "          3.2877e-02, -1.6847e-02, -1.1390e-02, -2.7755e-03, -3.0082e-02,\n",
       "          5.8287e-02,  1.1507e-02, -4.2968e-03, -1.0105e-02,  6.5570e-02,\n",
       "          1.3708e-03,  2.5616e-02, -1.4853e-02, -6.8747e-03,  1.7784e-02,\n",
       "          4.4831e-03, -3.7787e-03, -2.5504e-02, -5.9252e-02, -4.0941e-03,\n",
       "         -7.9554e-03,  3.0800e-03, -3.2719e-04,  4.1981e-02,  3.0262e-02,\n",
       "         -9.9765e-03, -4.9743e-02, -3.9645e-02, -2.6110e-02,  5.6602e-03,\n",
       "         -2.2802e-02, -1.9209e-02,  2.9630e-02, -1.2227e-02, -2.1206e-02,\n",
       "         -1.7226e-02, -1.6197e-03,  1.6194e-02, -4.0741e-02,  9.8773e-03,\n",
       "          3.1492e-02,  2.3636e-02, -3.5696e-02, -5.8127e-03, -2.0900e-02,\n",
       "          6.9528e-03,  1.6408e-03, -1.2193e-03,  4.4752e-02, -1.8349e-02,\n",
       "         -1.3311e-02,  1.6464e-02,  2.5576e-02,  2.8433e-02,  1.7528e-02,\n",
       "          1.8074e-02,  2.4653e-02, -5.3224e-02,  7.3445e-04, -1.3996e-02,\n",
       "          2.1466e-02, -1.3120e-02, -1.1009e-02,  4.0050e-02, -9.0814e-03,\n",
       "          2.8567e-02, -6.2081e-03,  3.4907e-02,  8.2342e-03, -1.6872e-02,\n",
       "         -5.4391e-02,  2.5006e-02,  8.7676e-02, -5.7883e-03, -1.7396e-02,\n",
       "          2.3762e-02,  1.4091e-02,  4.5205e-02,  1.7073e-02,  2.3916e-02,\n",
       "         -1.8069e-02,  1.4957e-01, -5.4058e-02, -3.3488e-03, -2.2275e-02,\n",
       "         -1.2764e-02, -8.7231e-03,  5.2909e-02,  1.8472e-02,  2.4544e-03,\n",
       "          1.3129e-03, -1.4439e-02, -1.6052e-02, -1.3819e-02,  3.1551e-03,\n",
       "          4.9861e-02,  6.7954e-03,  1.0688e-02, -1.4164e-03, -2.1036e-03,\n",
       "         -1.7890e-03, -5.2970e-03,  1.9557e-02,  7.1673e-03,  1.9388e-02,\n",
       "         -4.5418e-02, -1.1394e-02,  1.4939e-02, -7.7482e-03,  3.2199e-03,\n",
       "          1.0651e-02, -2.0988e-02,  5.1502e-02, -3.7455e-02, -1.8970e-02,\n",
       "         -9.8148e-03, -2.5015e-02,  2.4409e-02,  1.8763e-02,  3.5166e-02,\n",
       "          1.4199e-02,  1.1672e-02,  4.6718e-05, -1.2218e-02, -4.3527e-02,\n",
       "          1.8314e-02,  2.6936e-02, -5.0753e-02,  5.4167e-02, -9.8058e-03,\n",
       "          4.4024e-02,  4.6761e-02, -2.0138e-02, -5.5006e-02,  3.0078e-02,\n",
       "         -1.4046e-02,  1.6723e-01,  6.6372e-03, -3.9219e-02, -6.8728e-03,\n",
       "          6.2863e-03, -4.8055e-02, -1.5515e-02,  3.9459e-02, -1.1787e-02,\n",
       "          1.4829e-03, -1.0524e-02, -3.9896e-02,  1.1163e-02, -7.8098e-02,\n",
       "         -3.0232e-02, -4.1728e-02,  1.0351e-02, -2.4178e-02,  1.7776e-02,\n",
       "          1.1160e-02,  4.1927e-02, -3.0135e-02, -3.3255e-02, -2.5306e-02,\n",
       "          3.4820e-03,  1.9491e-02,  4.0108e-02,  2.6657e-02,  8.8106e-03,\n",
       "         -3.3932e-03, -1.8784e-02, -4.2418e-02,  5.7793e-02,  3.2260e-03,\n",
       "          4.6643e-02,  7.3860e-02, -1.7085e-02,  2.4312e-02,  5.1813e-05,\n",
       "         -6.0694e-02, -3.1265e-02, -5.8879e-03, -3.7228e-03, -3.4064e-02,\n",
       "         -5.3714e-02,  3.0489e-02,  1.8599e-02,  3.9148e-02,  1.1994e-02,\n",
       "          2.2577e-04,  1.0332e-02, -3.4379e-02,  6.1282e-02,  2.5935e-02,\n",
       "         -3.0369e-02,  7.3511e-02, -4.7710e-02,  2.4879e-02, -4.5111e-03,\n",
       "         -5.3721e-02, -3.6933e-03,  3.9060e-02,  2.7005e-02,  1.1995e-03,\n",
       "         -1.0788e-02, -2.4786e-02,  6.2904e-03, -1.0903e-02,  3.2191e-02,\n",
       "         -3.2052e-03,  4.8847e-03, -2.7337e-02, -1.8090e-02, -3.8773e-02,\n",
       "         -5.1707e-02, -2.3357e-02, -4.9173e-02,  4.9560e-02, -1.1259e-02,\n",
       "         -2.8557e-02,  3.4502e-02,  4.1570e-02,  1.2553e-02,  4.1773e-02,\n",
       "         -3.0382e-02, -3.6743e-02,  7.8641e-03, -2.6942e-02,  3.8504e-02,\n",
       "          1.3169e-02, -2.7578e-02, -2.0252e-02, -1.7956e-02, -3.7903e-02,\n",
       "          3.9641e-02, -4.7128e-02, -3.8402e-02, -9.7657e-03, -6.2052e-02,\n",
       "         -4.5354e-02, -3.4833e-02, -1.2541e-02, -1.1423e-02, -6.6859e-02,\n",
       "         -5.7571e-03, -9.2420e-03,  5.1104e-03,  6.7654e-03,  2.3139e-02,\n",
       "          1.4704e-02, -1.8602e-02,  1.0493e-02, -2.3245e-02, -1.4232e-02,\n",
       "         -4.0283e-03, -7.8815e-03, -1.7669e-02, -3.8235e-02,  5.7768e-02,\n",
       "          1.7646e-02,  7.3878e-03,  2.8385e-02,  1.1192e-02,  1.2610e-02,\n",
       "         -1.5433e-02,  5.2807e-02, -2.8414e-02, -1.6051e-02,  6.8229e-03,\n",
       "         -2.1126e-02,  1.2496e-02, -4.1585e-02,  1.1831e-02,  8.0263e-02,\n",
       "         -1.3500e-03,  2.3734e-02]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['image_embeds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['text_embeds'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['image_embeds'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_per_image = outputs.logits_per_image\n",
    "probs = logits_per_image.softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[24.5701, 19.3049]], grad_fn=<TBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_per_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (CLIPFeatureExtractor, \n",
    "                          CLIPTokenizer,\n",
    "                          CLIPProcessor,\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "inputs['pixel_values'].shape\n",
    "print('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.clip.tokenization_clip.CLIPTokenizer"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CLIPTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 402M/402M [01:16<00:00, 5.53MiB/s]\n"
     ]
    }
   ],
   "source": [
    "import clip\n",
    "clip_model, preprocess = clip.load(\"RN50x4\", jit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "image_np = torch.tensor(np.array(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 480, 640, 3])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_np[None,:,:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 480, 640])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_np.view(1, 3, 480, 640).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
