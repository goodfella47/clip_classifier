{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is based on MMBT https://arxiv.org/abs/1909.02950\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from madgrad import MADGRAD\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
    "\n",
    "import pickle\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import copy\n",
    "\n",
    "import clip\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoImageProcessor,\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    MMBTConfig,\n",
    "    MMBTModel,\n",
    "    MMBTForClassification,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    set_seed,\n",
    "    CLIPVisionModel,\n",
    "    CLIPTextModel,\n",
    "    CLIPTextModelWithProjection,\n",
    "    VisionTextDualEncoderModel\n",
    "\n",
    ")\n",
    "from transformers import CLIPModel, CLIPProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JsonDataset(Dataset):\n",
    "    def __init__(self, data_path, data_processor, num_bins, max_seq_length):\n",
    "        self.data = [json.loads(l) for l in open(data_path)]\n",
    "        self.data_dir = os.path.dirname(data_path)\n",
    "        self.data_processor = data_processor\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.num_bins = num_bins\n",
    "        self.bin_edges, self.labels = self.create_labels()\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        item = self.data[index]\n",
    "        caption = item[\"text\"][:self.max_seq_length]\n",
    "        img = Image.open(item[\"image_path\"]).convert(\"RGB\")\n",
    "        inputs = self.data_processor(caption=caption, image=img, return_tensors=\"pt\")\n",
    "        label = self.num_to_bin(float(item[\"price\"]))\n",
    "        return inputs, label\n",
    "        \n",
    "    def get_label_frequencies(self):\n",
    "        label_freqs = Counter()\n",
    "        for row in self.data:\n",
    "            label_freqs.update([row[\"label\"]])\n",
    "        return label_freqs\n",
    "    \n",
    "    def get_labels(self):\n",
    "        labels = []\n",
    "        for row in self.data:\n",
    "            labels.append(row[\"label\"])\n",
    "        return labels\n",
    "    \n",
    "    def create_labels(self,):\n",
    "        # Compute quantiles\n",
    "        quantiles = np.linspace(0, 1, self.num_bins + 1)\n",
    "        prices = [float(annot[\"price\"]) for annot in self.data]\n",
    "        bin_edges = np.quantile(prices, quantiles)\n",
    "        labels = [\"%.2f - %.2f\" % (bin_edges[i], bin_edges[i+1]) for i in range(self.num_bins)]\n",
    "        return bin_edges, labels\n",
    "        \n",
    "    def num_to_bin(self, num):\n",
    "        for i in range(self.num_bins):\n",
    "            if num >= self.bin_edges[i] and num < self.bin_edges[i+1]:\n",
    "                return self.labels[i]\n",
    "        return self.labels[-1]\n",
    "\n",
    "   \n",
    "def collate_fn(batch):\n",
    "    lens = [len(row[\"sentence\"]) for row in batch]\n",
    "    bsz, max_seq_len = len(batch), max(lens)\n",
    "\n",
    "    mask_tensor = torch.zeros(bsz, max_seq_len, dtype=torch.long)\n",
    "    text_tensor = torch.zeros(bsz, max_seq_len, dtype=torch.long)\n",
    "\n",
    "    for i_batch, (input_row, length) in enumerate(zip(batch, lens)):\n",
    "        text_tensor[i_batch, :length] = input_row[\"sentence\"]\n",
    "        mask_tensor[i_batch, :length] = 1\n",
    "\n",
    "    img_tensor = torch.stack([row[\"image\"] for row in batch])\n",
    "    tgt_tensor = torch.stack([row[\"label\"] for row in batch])\n",
    "    img_start_token = torch.stack([row[\"image_start_token\"] for row in batch])\n",
    "    img_end_token = torch.stack([row[\"image_end_token\"] for row in batch])\n",
    "\n",
    "    return text_tensor, mask_tensor, img_tensor, img_start_token, img_end_token, tgt_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-base-patch32 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'logit_scale', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_projection.weight', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'visual_projection.weight', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.5.self_attn.q_proj.bias']\n",
      "- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "image_features_size = 768\n",
    "image_input_size = 256\n",
    "\n",
    "vision_model = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "# freeze clip weights, wont be finetuning them\n",
    "for p in vision_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "class ClipEncoder(nn.Module):\n",
    "    def __init__(self, num_embeds=1, num_features=image_features_size):\n",
    "        super().__init__()        \n",
    "        self.vision_model = vision_model\n",
    "        self.num_embeds = num_embeds\n",
    "        self.num_features = num_features\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = self.vision_model(x)\n",
    "        # out = out.view(-1, self.num_embeds, self.num_features).float()\n",
    "        return out  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright (c) Facebook, Inc. and its affiliates.\n",
    "# Copyright (c) HuggingFace Inc. team.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"PyTorch MMBT model.\"\"\"\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPooling, SequenceClassifierOutput\n",
    "from transformers.modeling_utils import ModuleUtilsMixin\n",
    "\n",
    "\n",
    "class ModalEmbeddings(nn.Module):\n",
    "    \"\"\"Generic Modal Embeddings which takes in an encoder, and a transformer embedding.\"\"\"\n",
    "\n",
    "    def __init__(self, config, encoder, embeddings):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.encoder = encoder\n",
    "        self.proj_embeddings = nn.Linear(config.modal_hidden_size, config.hidden_size)\n",
    "        self.position_embeddings = embeddings.position_embeddings\n",
    "        self.token_embedding = embeddings.token_embedding # use token_embedding instead of word_embeddings\n",
    "        self.dropout = nn.Dropout(p=config.dropout)\n",
    "\n",
    "    def forward(self, input_modal, start_token=None, end_token=None, position_ids=None, token_type_ids=None):\n",
    "        token_embeddings = self.proj_embeddings(self.encoder(input_modal))\n",
    "        seq_length = token_embeddings.size(1)\n",
    "\n",
    "        if start_token is not None:\n",
    "            start_token_embeds = self.token_embedding(start_token)\n",
    "            seq_length += 1\n",
    "            token_embeddings = torch.cat([start_token_embeds.unsqueeze(1), token_embeddings], dim=1)\n",
    "\n",
    "        if end_token is not None:\n",
    "            end_token_embeds = self.token_embedding(end_token)\n",
    "            seq_length += 1\n",
    "            token_embeddings = torch.cat([token_embeddings, end_token_embeds.unsqueeze(1)], dim=1)\n",
    "\n",
    "        if position_ids is None:\n",
    "            position_ids = torch.arange(seq_length, dtype=torch.long, device=input_modal.device)\n",
    "            position_ids = position_ids.unsqueeze(0).expand(input_modal.size(0), seq_length)\n",
    "\n",
    "        # if token_type_ids is None:\n",
    "        #     token_type_ids = torch.zeros(\n",
    "        #         (input_modal.size(0), seq_length), dtype=torch.long, device=input_modal.device\n",
    "        #     )\n",
    "\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        embeddings = token_embeddings + position_embeddings\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class MMBTModel(nn.Module, ModuleUtilsMixin):\n",
    "    def __init__(self, config, transformer, encoder):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.transformer = transformer\n",
    "        self.modal_encoder = ModalEmbeddings(config, encoder, transformer.embeddings)\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_modal,\n",
    "        input_ids=None,\n",
    "        modal_start_tokens=None,\n",
    "        modal_end_tokens=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        modal_token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        modal_position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "        elif input_ids is not None:\n",
    "            input_txt_shape = input_ids.size()\n",
    "        elif inputs_embeds is not None:\n",
    "            input_txt_shape = inputs_embeds.size()[:-1]\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
    "\n",
    "        modal_embeddings = self.modal_encoder(\n",
    "            input_modal,\n",
    "            start_token=modal_start_tokens,\n",
    "            end_token=modal_end_tokens,\n",
    "            position_ids=modal_position_ids,\n",
    "            token_type_ids=modal_token_type_ids,\n",
    "        )\n",
    "\n",
    "        input_modal_shape = modal_embeddings.size()[:-1]\n",
    "\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.ones(input_txt_shape, dtype=torch.long, device=device)\n",
    "\n",
    "        txt_embeddings = self.transformer.embeddings(\n",
    "            input_ids=input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds\n",
    "        )\n",
    "\n",
    "        embedding_output = torch.cat([modal_embeddings, txt_embeddings], 1)\n",
    "\n",
    "        input_shape = embedding_output.size()[:-1]\n",
    "\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones(input_shape, device=device)\n",
    "        else:\n",
    "            attention_mask = torch.cat(\n",
    "                [torch.ones(input_modal_shape, device=device, dtype=torch.long), attention_mask], dim=1\n",
    "            )\n",
    "        if encoder_attention_mask is None:\n",
    "            encoder_attention_mask = torch.ones(input_shape, device=device)\n",
    "        else:\n",
    "            encoder_attention_mask = torch.cat(\n",
    "                [torch.ones(input_modal_shape, device=device), encoder_attention_mask], dim=1\n",
    "            )\n",
    "\n",
    "        extended_attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)\n",
    "        encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n",
    "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
    "\n",
    "        encoder_outputs = self.transformer.encoder(\n",
    "            embedding_output,\n",
    "            attention_mask=extended_attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_extended_attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = encoder_outputs[0]\n",
    "        pooled_output = self.transformer.pooler(sequence_output)\n",
    "\n",
    "        if not return_dict:\n",
    "            return (sequence_output, pooled_output) + encoder_outputs[1:]\n",
    "\n",
    "        return BaseModelOutputWithPooling(\n",
    "            last_hidden_state=sequence_output,\n",
    "            pooler_output=pooled_output,\n",
    "            hidden_states=encoder_outputs.hidden_states,\n",
    "            attentions=encoder_outputs.attentions,\n",
    "        )\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.embeddings.word_embeddings\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.embeddings.word_embeddings = value\n",
    "\n",
    "\n",
    "class MMBTForClassification(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, config, transformer, encoder):\n",
    "        super().__init__()\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.mmbt = MMBTModel(config, transformer, encoder)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_modal,\n",
    "        input_ids=None,\n",
    "        modal_start_tokens=None,\n",
    "        modal_end_tokens=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        modal_token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        modal_position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.mmbt(\n",
    "            input_modal=input_modal,\n",
    "            input_ids=input_ids,\n",
    "            modal_start_tokens=modal_start_tokens,\n",
    "            modal_end_tokens=modal_end_tokens,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            modal_token_type_ids=modal_token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            modal_position_ids=modal_position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.num_labels == 1:\n",
    "                #  We are doing regression\n",
    "                loss_fct = MSELoss()\n",
    "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            else:\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified clip code to fit MMBT\n",
    "\n",
    "from typing import Any, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers.models.clip.modeling_clip import (CLIPTextEmbeddings, \n",
    "                                                    _expand_mask, \n",
    "                                                    _make_causal_mask,\n",
    "                                                    CLIPEncoder)\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPooling\n",
    "from transformers import CLIPTextConfig\n",
    "\n",
    "\n",
    "class CLIPPooler(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        \n",
    "    def forward(self, hidden_states: torch.Tensor):\n",
    "\n",
    "        return self.layer_norm(\n",
    "            hidden_states[\n",
    "                torch.arange(hidden_states.shape[0], device=hidden_states.device),\n",
    "                hidden_states.argmax(dim=-1).to(dtype=torch.int, device=hidden_states.device),\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "class CLIPTextEmbeddingsCustom(nn.Module):\n",
    "    def __init__(self, config: CLIPTextConfig):\n",
    "        super().__init__()\n",
    "        embed_dim = config.hidden_size\n",
    "\n",
    "        self.token_embedding = nn.Embedding(config.vocab_size, embed_dim)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, embed_dim)\n",
    "\n",
    "        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n",
    "        self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        seq_length = input_ids.shape[-1] if input_ids is not None else inputs_embeds.shape[-2]\n",
    "\n",
    "        if position_ids is None:\n",
    "            position_ids = self.position_ids[:, :seq_length]\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.token_embedding(input_ids)\n",
    "\n",
    "        position_embeddings = self.position_embedding(position_ids)\n",
    "        embeddings = inputs_embeds + position_embeddings\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class CLIPTextTransformer(nn.Module):\n",
    "    def __init__(self, config: CLIPTextConfig, add_pooling_layer=True):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        embed_dim = config.hidden_size\n",
    "        self.embeddings = CLIPTextEmbeddingsCustom(config)\n",
    "        self.encoder = CLIPEncoder(config)\n",
    "        self.pooler = CLIPPooler(config) if add_pooling_layer else None\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ):\n",
    "\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if input_ids is None:\n",
    "            raise ValueError(\"You have to specify input_ids\")\n",
    "\n",
    "        input_shape = input_ids.size()\n",
    "        input_ids = input_ids.view(-1, input_shape[-1])\n",
    "\n",
    "        hidden_states = self.embeddings(input_ids=input_ids, position_ids=position_ids)\n",
    "\n",
    "        # CLIP's text model uses causal mask, prepare it here.\n",
    "        # https://github.com/openai/CLIP/blob/cfcffb90e69f37bf2ff1e988237a0fbe41f33c04/clip/model.py#L324\n",
    "        causal_attention_mask = _make_causal_mask(input_shape, hidden_states.dtype, device=hidden_states.device)\n",
    "        # expand attention_mask\n",
    "        if attention_mask is not None:\n",
    "            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
    "            attention_mask = _expand_mask(attention_mask, hidden_states.dtype)\n",
    "\n",
    "        encoder_outputs = self.encoder(\n",
    "            inputs_embeds=hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            causal_attention_mask=causal_attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = encoder_outputs[0]\n",
    "        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n",
    "\n",
    "        # text_embeds.shape = [batch_size, sequence_length, transformer.width]\n",
    "        # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "        # casting to torch.int for onnx compatibility: argmax doesn't support int64 inputs with opset 14\n",
    "\n",
    "\n",
    "        if not return_dict:\n",
    "            return (sequence_output, pooled_output) + encoder_outputs[1:]\n",
    "\n",
    "\n",
    "        return BaseModelOutputWithPooling(\n",
    "            last_hidden_state=sequence_output,\n",
    "            pooler_output=pooled_output,\n",
    "            hidden_states=encoder_outputs.hidden_states,\n",
    "            attentions=encoder_outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "# image_processor = AutoImageProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "model_name = \"openai/clip-vit-base-patch32\"\n",
    "# text_model_name = \"roberta-base\"\n",
    "transformer_config = CLIPTextConfig.from_pretrained(model_name) \n",
    "transformer = CLIPTextTransformer(transformer_config)\n",
    "# transformer = AutoModel.from_pretrained(text_model_name, config=transformer_config)\n",
    "# transformer_clip =  CLIPTextModel.from_pretrained(vision_model_name)\n",
    "img_encoder = ClipEncoder()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "config = MMBTConfig(transformer_config, num_labels=10, modal_hidden_size=512)\n",
    "model = MMBTForClassification(config, transformer, img_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MMBTForClassification(\n",
       "  (mmbt): MMBTModel(\n",
       "    (transformer): CLIPTextTransformer(\n",
       "      (embeddings): CLIPTextEmbeddingsCustom(\n",
       "        (token_embedding): Embedding(49408, 512)\n",
       "        (position_embeddings): Embedding(77, 512)\n",
       "      )\n",
       "      (encoder): CLIPEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x CLIPEncoderLayer(\n",
       "            (self_attn): CLIPAttention(\n",
       "              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): CLIPMLP(\n",
       "              (activation_fn): QuickGELUActivation()\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): CLIPPooler(\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (modal_encoder): ModalEmbeddings(\n",
       "      (encoder): ClipEncoder(\n",
       "        (vision_model): CLIPVisionModel(\n",
       "          (vision_model): CLIPVisionTransformer(\n",
       "            (embeddings): CLIPVisionEmbeddings(\n",
       "              (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "              (position_embedding): Embedding(50, 768)\n",
       "            )\n",
       "            (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder): CLIPEncoder(\n",
       "              (layers): ModuleList(\n",
       "                (0-11): 12 x CLIPEncoderLayer(\n",
       "                  (self_attn): CLIPAttention(\n",
       "                    (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  )\n",
       "                  (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (mlp): CLIPMLP(\n",
       "                    (activation_fn): QuickGELUActivation()\n",
       "                    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                  )\n",
       "                  (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (proj_embeddings): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (position_embeddings): Embedding(77, 512)\n",
       "      (token_embedding): Embedding(49408, 512)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       "  (classifier): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
